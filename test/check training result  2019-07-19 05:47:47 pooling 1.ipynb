{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Test KPRN Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"2019-07-19 05:47:47\"\n",
    "MODEL_DIR = \"../logs/{}\".format(TEST_CODE)\n",
    "CHOSEN_EPOCH = 5\n",
    "\n",
    "MAX_SEED_NUM = 3\n",
    "MAX_RELATION_NUM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = sorted(os.listdir(MODEL_DIR))\n",
    "choosen_weight = \"{}/{}\".format(MODEL_DIR, trained_weights[CHOSEN_EPOCH - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 08:49:04.740978 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 08:49:04.748435 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0801 08:49:04.771515 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0801 08:49:04.886361 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0801 08:49:04.896678 140577186756352 deprecation.py:506] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0801 08:49:05.287362 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0801 08:49:06.113663 140577186756352 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 08:49:06.124999 140577186756352 deprecation.py:323] From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(choosen_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ratings_re = open(\"../data/ratings_re.csv\").readlines()\n",
    "file_triples_idx = open(\"../data/triples_idx.txt\").readlines()\n",
    "file_moviesIdx = open(\"../data/moviesIdx.txt\").readlines()\n",
    "file_types = open(\"../data/types.txt\").readlines()\n",
    "file_entities = open(\"../data/entities.txt\").readlines()\n",
    "file_relations = open(\"../data/relations.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_entity_to_name():\n",
    "\n",
    "    entity_id_to_name = {}\n",
    "    for line in file_moviesIdx:\n",
    "        movie_title, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = movie_title\n",
    "\n",
    "    for line in file_entities:\n",
    "        entity_name, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = entity_name\n",
    "\n",
    "    return entity_id_to_name\n",
    "\n",
    "def _get_movie_title_to_entity_type():\n",
    "\n",
    "    movie_title_to_entity_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        movie_title_to_entity_type[entity] = entity_type\n",
    "\n",
    "    return movie_title_to_entity_type\n",
    "\n",
    "def _get_entity_list_with_type():\n",
    "\n",
    "    entity_list_with_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        if entity_type not in entity_list_with_type:\n",
    "            entity_list_with_type[entity_type] = []\n",
    "        entity_list_with_type[entity_type].append(entity)\n",
    "\n",
    "    return entity_list_with_type\n",
    "\n",
    "REL_ID_END = '200030'\n",
    "def _get_relation_to_name():\n",
    "\n",
    "    # Create relation id to name mapping\n",
    "    relation_id_to_name = {}\n",
    "    for line in file_relations:\n",
    "        relation_name, relation_id = line.strip().split()\n",
    "        relation_id = int(relation_id)\n",
    "        relation_id += 200000\n",
    "\n",
    "        # last 2 relation : spouse and relative has no inverse\n",
    "        if relation_id < 200023:\n",
    "            relation_id_to_name[str(relation_id + 1)] = relation_name + \"_inverse\"\n",
    "\n",
    "        relation_id_to_name[str(relation_id)] = relation_name\n",
    "\n",
    "    relation_id_to_name[REL_ID_END] = \"END\"\n",
    "    return relation_id_to_name\n",
    "\n",
    "def _get_entity_type_to_id():\n",
    "\n",
    "    list_of_type = [\n",
    "        \"Category\",\n",
    "        \"Company\",\n",
    "        \"Country\",\n",
    "        \"Genre\",\n",
    "        \"Movie\",\n",
    "        \"Person\",\n",
    "        \"User\",\n",
    "        \"#PAD_TOKEN\",\n",
    "        \"#UNK_ENTITY_TYPE\",\n",
    "    ]\n",
    "\n",
    "    entity_type_to_id = {list_of_type[i]: i for i in range(0, len(list_of_type))}\n",
    "    return entity_type_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_to_entity_type = _get_movie_title_to_entity_type()\n",
    "entity_list_with_type = _get_entity_list_with_type()\n",
    "entity_id_to_name = _get_entity_to_name()\n",
    "relation_id_to_name = _get_relation_to_name()\n",
    "entity_type_to_id = _get_entity_type_to_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ENTITY_ID_PADDING = 500000\n",
    "def get_type_from_entity_id(entity_id):\n",
    "    # user\n",
    "    if int(entity_id) > USER_ENTITY_ID_PADDING:\n",
    "        return \"User\"\n",
    "    elif entity_id_to_name[entity_id] in movie_title_to_entity_type:\n",
    "        return movie_title_to_entity_type[entity_id_to_name[entity_id]]\n",
    "    else:\n",
    "        return \"#UNK_ENTITY_TYPE\"\n",
    "    \n",
    "def path_to_string(paths):\n",
    "    string_paths = []\n",
    "    for path in paths:\n",
    "\n",
    "        entity_string = []\n",
    "        for entity in path.split():\n",
    "            entity_string.append(get_entity_name(str(entity)))\n",
    "        entity_string = \" -> \".join(entity_string)\n",
    "\n",
    "        string_paths.append(entity_string)\n",
    "\n",
    "    return string_paths\n",
    "\n",
    "def get_entity_name(entity_id):\n",
    "\n",
    "    if entity_id in entity_id_to_name:\n",
    "        return entity_id_to_name[entity_id]\n",
    "    elif entity_id in relation_id_to_name:\n",
    "        return relation_id_to_name[entity_id]\n",
    "    else:\n",
    "        return \"user_{}\".format(entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Load KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KG from cache\n",
    "kg_path = pickle.load(open(\"../data/cache_kg_path\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Prepare path for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRREGULAR_RELATION = ['200024', '200025']\n",
    "NUM_OF_ITEMS = len(file_moviesIdx)\n",
    "\n",
    "def _generate_path_from_entity_to_all_others(entity_id, keep_relation_ratio=0.5, max_relation_num=30, max_entity_per_relation=1):\n",
    "\n",
    "    generated_paths = []\n",
    "\n",
    "    # Get direct relation attached to entity1\n",
    "    r1 = kg_path[entity_id]\n",
    "\n",
    "    # List all possible item-entities\n",
    "    all_item_entities = [str(x) for x in range(0, NUM_OF_ITEMS)]\n",
    "    for e2_id in all_item_entities:\n",
    "\n",
    "        # Skip if e1 == e2 (path to itself)\n",
    "        if e2_id == entity_id:\n",
    "            continue\n",
    "\n",
    "        # Get relations attached to entity2\n",
    "        r2 = kg_path[e2_id]\n",
    "\n",
    "        # Downsample intersect relation\n",
    "        intersect_relations = list(set(r1.keys()).intersection(set(r2.keys())))\n",
    "        if len(intersect_relations) < max_relation_num:\n",
    "            n_intersect_relation = len(intersect_relations)\n",
    "        else:\n",
    "            n_intersect_relation = max(max_relation_num, int(len(intersect_relations) * keep_relation_ratio))\n",
    "\n",
    "        # Find intersect between relation 1 and relation 2\n",
    "        np.random.shuffle(intersect_relations)\n",
    "        for relation in intersect_relations[:n_intersect_relation]:\n",
    "\n",
    "            # Find entities attached to relation (non-item entities)\n",
    "            intersect_entity = list(set(r1[relation]).intersection(set(r2[relation])))\n",
    "\n",
    "            # Down sample intersecting entities\n",
    "            np.random.shuffle(intersect_entity)\n",
    "            for mid_entity in intersect_entity[:max_entity_per_relation]:\n",
    "\n",
    "                # Format path and add to result\n",
    "                inverse_relation = relation if (relation in IRREGULAR_RELATION) else str(int(relation) + 1)\n",
    "                path = \"{} {} {} {} {}\".format(entity_id, relation, mid_entity, inverse_relation, e2_id)\n",
    "                generated_paths.append(path)\n",
    "\n",
    "    return generated_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_ID_GIVEN_GOOD_RATING = '200027'\n",
    "def _generate_all_path_from_user(user_id, max_seed_num=10, max_seed_ratio=0.2):\n",
    "\n",
    "    # List down all user interacted items\n",
    "    user_interacted_items = []\n",
    "    for relation in kg_path[user_id]:\n",
    "        user_interacted_items += kg_path[user_id][relation]\n",
    "    user_interacted_items = sorted(set(user_interacted_items))\n",
    "\n",
    "    # Downsample seeds (item interacted)\n",
    "    if len(user_interacted_items) < max_seed_num:\n",
    "        n_seed = len(user_interacted_items)\n",
    "    else:\n",
    "        n_seed = max(max_seed_num, int(len(user_interacted_items) * max_seed_ratio))\n",
    "    np.random.shuffle(user_interacted_items)\n",
    "    user_interacted_items = user_interacted_items[:n_seed]\n",
    "\n",
    "    # ====== Threading ======\n",
    "    user_paths = []\n",
    "    threads = []\n",
    "    for i in range(0, n_seed):\n",
    "\n",
    "        # Split items id equally\n",
    "        thread_items = user_interacted_items[i::n_seed]\n",
    "        thread = threading.Thread(target=_run_thread, args=(i, thread_items, user_paths))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all thread to finish\n",
    "    for i in range(0, n_seed):\n",
    "        threads[i].join()\n",
    "\n",
    "    # Reformat : add user id and 'REL_ID_GIVEN_GOOD_RATING'\n",
    "    user_paths = [\"{} {} {}\".format(user_id, REL_ID_GIVEN_GOOD_RATING, x) for x in user_paths]\n",
    "    return user_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run path preparation using thread to reduce time\n",
    "def _run_thread(thread_id, thread_items, result):\n",
    "    for item_id in (thread_items):\n",
    "        result += _generate_path_from_entity_to_all_others(item_id, keep_relation_ratio=0.5,\n",
    "                                                           max_relation_num=MAX_RELATION_NUM, max_entity_per_relation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reformat_user_path(user_paths):\n",
    "\n",
    "    new_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for path in user_paths:\n",
    "        e1, r1, e2, r2, e3, r3, e4 = path.strip().split()\n",
    "\n",
    "        t1 = entity_type_to_id[get_type_from_entity_id(e1)]\n",
    "        t2 = entity_type_to_id[get_type_from_entity_id(e2)]\n",
    "        t3 = entity_type_to_id[get_type_from_entity_id(e3)]\n",
    "        t4 = entity_type_to_id[get_type_from_entity_id(e4)]\n",
    "\n",
    "        r4 = REL_ID_END\n",
    "\n",
    "        entity_rated = kg_path[e1][REL_ID_GIVEN_GOOD_RATING]\n",
    "        label = 1 if e4 in entity_rated else 0\n",
    "\n",
    "        new_paths.append([\n",
    "            [e1, t1, r1],\n",
    "            [e2, t2, r2],\n",
    "            [e3, t3, r3],\n",
    "            [e4, t4, r4],\n",
    "        ])\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(new_paths).astype('int'), np.array(labels).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=-1):\n",
    "    \"\"\"\n",
    "    max_pooling_size -1 means without pooling\n",
    "    \"\"\"\n",
    "    if max_pooling_size > 0:\n",
    "        return _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=max_pooling_size, get_best=get_best)\n",
    "    else:\n",
    "        return _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=get_best)\n",
    "\n",
    "def _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on y_pred scores, \n",
    "    pick all path in sorted X_test until the amount of unique items is equal to k\n",
    "    \"\"\"\n",
    "    choosen_paths = []\n",
    "    choosen_items = set()\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "        choosen_paths.append((path, score))\n",
    "        choosen_items.add(path[3][0])  # Add the last item\n",
    "\n",
    "        if len(choosen_items) >= k:\n",
    "            break\n",
    "\n",
    "    return choosen_paths, choosen_items\n",
    "\n",
    "def _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=5, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on averaged y_pred scores,\n",
    "    pick 'max_pooling_size' paths for each item, rank item based on average score, pick top k items.\n",
    "    \"\"\"\n",
    "    paths_group_by_items = {}\n",
    "    score_group_by_items = {}\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "\n",
    "        ending_item = path[3][0]  # The last item before END RELATION\n",
    "\n",
    "        if ending_item not in paths_group_by_items:\n",
    "            paths_group_by_items[ending_item] = []\n",
    "            score_group_by_items[ending_item] = []\n",
    "\n",
    "        # Pool the paths and score\n",
    "        if len(paths_group_by_items[ending_item]) < max_pooling_size:\n",
    "            paths_group_by_items[ending_item].append((path, score))\n",
    "            score_group_by_items[ending_item].append(score)\n",
    "\n",
    "    # Calculate average\n",
    "    for item in score_group_by_items:\n",
    "        score_group_by_items[item] = sum(score_group_by_items[item]) / len(score_group_by_items[item])\n",
    "\n",
    "    # Sort pooled result\n",
    "    sorted_average_score = sorted(score_group_by_items.items(), key=lambda kv: kv[1], reverse=get_best)\n",
    "    choosen_items = {key for key, v in sorted_average_score[:k]}\n",
    "\n",
    "    # Filter top-k items with highest average score\n",
    "    choosen_paths = []\n",
    "    for item_id in choosen_items:\n",
    "        choosen_paths += paths_group_by_items[item_id]\n",
    "\n",
    "    return choosen_paths, choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestion(user_id, k=10, max_pooling_size=-1):\n",
    "\n",
    "    user_paths = _generate_all_path_from_user(user_id, max_seed_num=MAX_SEED_NUM, max_seed_ratio=0.3)\n",
    "    X_test, _ = _reformat_user_path(user_paths)\n",
    "\n",
    "    y_pred = model.predict(X_test, batch_size=2048, verbose=1)\n",
    "\n",
    "    top_paths, top_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=max_pooling_size)\n",
    "    worst_path, worst_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=False, max_pooling_size=max_pooling_size)\n",
    "\n",
    "    return top_paths, top_choosen_items, worst_path, worst_choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user_id):\n",
    "    return set(kg_path[user_id][REL_ID_GIVEN_GOOD_RATING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(pred, truth, k=10):\n",
    "\n",
    "    # Make sure same type\n",
    "    pred = {str(x) for x in pred}\n",
    "    truth = {str(x) for x in truth}\n",
    "\n",
    "    intersect = pred.intersection(truth)\n",
    "\n",
    "    len_intersect = len(intersect)\n",
    "    len_truth = len(truth) if 0 < len(truth) <= k else k\n",
    "\n",
    "    return intersect, len_intersect / len_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasoning_paths(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reason = \" -> \".join(reason_path)\n",
    "            reasoning_paths.append(reason)\n",
    "    return reasoning_paths\n",
    "\n",
    "\n",
    "def get_reasoning_paths_df(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reasoning_paths.append(reason_path)\n",
    "    return pd.DataFrame(reasoning_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare_prediction_truth(predictions, truths):\n",
    "\n",
    "    predictions = [get_entity_name(str(p)) for p in predictions]\n",
    "    truths = [get_entity_name(str(t)) for t in truths]\n",
    "\n",
    "    print(\"Predictions : \")\n",
    "    for p in sorted(predictions):\n",
    "        is_watched = \"watched\" if str(p) in truths else \"nope\"\n",
    "        print(\"{} > {}\".format(p, is_watched))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Truth : \")\n",
    "    for t in sorted(truths):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278472/278472 [==============================] - 1s 5us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/30 [03:09<1:31:32, 189.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78692/78692 [==============================] - 0s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [03:42<1:06:30, 142.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63422/63422 [==============================] - 0s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [03:59<47:08, 104.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116693/116693 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [05:44<45:28, 104.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016089/1016089 [==============================] - 4s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 5/30 [12:02<1:17:48, 186.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136947/136947 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 6/30 [14:43<1:11:36, 179.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116423/116423 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 7/30 [15:47<55:28, 144.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306524/306524 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 8/30 [17:31<48:36, 132.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92315/92315 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 9/30 [18:22<37:48, 108.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105052/105052 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 10/30 [19:20<31:01, 93.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240725/240725 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 11/30 [20:59<30:00, 94.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367718/367718 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 12/30 [23:23<32:48, 109.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111782/111782 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 13/30 [24:02<25:02, 88.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75478/75478 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 14/30 [24:20<17:56, 67.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68369/68369 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 15/30 [25:06<15:11, 60.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85671/85671 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 16/30 [25:29<11:31, 49.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578270/578270 [==============================] - 2s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 17/30 [28:40<19:54, 91.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509989/509989 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 18/30 [32:26<26:26, 132.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310382/310382 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 19/30 [34:09<22:39, 123.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65985/65985 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 20/30 [34:34<15:38, 93.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66995/66995 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 21/30 [34:50<10:35, 70.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266489/266489 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 22/30 [37:28<12:54, 96.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324994/324994 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 23/30 [39:15<11:38, 99.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106883/106883 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 24/30 [40:03<08:25, 84.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102615/102615 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 25/30 [40:19<05:18, 63.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214405/214405 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 26/30 [41:19<04:10, 62.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85346/85346 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 27/30 [42:21<03:07, 62.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645689/645689 [==============================] - 2s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 28/30 [44:41<02:51, 85.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89656/89656 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 29/30 [45:27<01:13, 73.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437424/1437424 [==============================] - 4s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [53:17<00:00, 192.56s/it]\n"
     ]
    }
   ],
   "source": [
    "k_suggestions = 10\n",
    "n_users = 30\n",
    "\n",
    "sample_user = np.random.randint(500001, 630000, n_users)\n",
    "sample_user = [str(x) for x in sample_user]\n",
    "\n",
    "top_paths = []\n",
    "top_items = []\n",
    "worst_paths = []\n",
    "worst_items = []\n",
    "\n",
    "truth_items = []\n",
    "\n",
    "n_paths = []\n",
    "intersects = []\n",
    "scores = []\n",
    "\n",
    "all_intersect = None\n",
    "all_union = None\n",
    "\n",
    "for user in tqdm(sample_user):\n",
    "    top_suggested_path, top_suggested_items, worst_suggested_path, worst_suggested_items = get_suggestion(user, k=k_suggestions, max_pooling_size=-1)\n",
    "    top_truth_items = get_top_truth(user)\n",
    "\n",
    "    intersect, score = check_precision(top_suggested_items, top_truth_items)\n",
    "\n",
    "    top_paths.append(top_suggested_path)\n",
    "    top_items.append(top_suggested_items)\n",
    "\n",
    "    worst_paths.append(worst_suggested_path)\n",
    "    worst_items.append(worst_suggested_items)\n",
    "\n",
    "    intersects.append(intersect)\n",
    "    truth_items.append(top_truth_items)\n",
    "\n",
    "    n_paths.append(len(top_suggested_path))\n",
    "    scores.append(score)\n",
    "\n",
    "    if all_intersect is None:\n",
    "        all_intersect = top_suggested_items\n",
    "    else:\n",
    "        all_intersect = all_intersect.intersection(top_suggested_items)\n",
    "\n",
    "    if all_union is None:\n",
    "        all_union = top_suggested_items\n",
    "    else:\n",
    "        all_union = all_union.union(top_suggested_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec@k score: 0.12309523809523809\n",
      "\n",
      "intersect\n",
      "set() 0\n",
      "\n",
      "union\n",
      "{9227, 18, 11796, 534, 6167, 2073, 9758, 546, 9251, 9254, 3111, 41, 10796, 7729, 11314, 5174, 5686, 4666, 10811, 1084, 1085, 5183, 1599, 4160, 12355, 14920, 11337, 1101, 4687, 90, 4700, 8797, 1630, 11871, 616, 8811, 9326, 12402, 10355, 10357, 9846, 4215, 3701, 11893, 3197, 10371, 8324, 7815, 7304, 11914, 652, 141, 1168, 13970, 10390, 156, 8861, 676, 4260, 12457, 2219, 6834, 6322, 14516, 2741, 12470, 14007, 1719, 2232, 8377, 3252, 7869, 7870, 5825, 2245, 14025, 14026, 5325, 2767, 10453, 14039, 5339, 741, 5355, 1774, 14062, 12016, 4854, 11002, 2298, 11006, 8959, 11008, 8962, 9994, 11536, 794, 10526, 5409, 2339, 1321, 1322, 2863, 6962, 5427, 1843, 10560, 11088, 3922, 12626, 9555, 854, 857, 8026, 12126, 12639, 8545, 2914, 12643, 3940, 3943, 1392, 2423, 5497, 12670, 14719, 15236, 7049, 8074, 909, 911, 1423, 6032, 9620, 11677, 14751, 10659, 9125, 2474, 15279, 1457, 1976, 9662, 2495, 959, 3518, 5058, 9155, 6092, 14285, 7631, 7632, 2514, 15323, 477, 5086, 3551, 8672, 8159, 8674, 11747, 13797, 4071, 9704, 4591, 5619, 11251, 12790, 8186, 5629, 5119} 171\n",
      "\n",
      "distinct rate\n",
      "0.57\n"
     ]
    }
   ],
   "source": [
    "print(\"Prec@k score:\", np.average(scores))\n",
    "# print(\"top_suggested_items:\", top_suggested_items)\n",
    "# print(\"truth_items:\", truth_items)\n",
    "\n",
    "print(\"\\nintersect\")\n",
    "print(all_intersect, len(all_intersect))\n",
    "print(\"\\nunion\")\n",
    "print(all_union, len(all_union))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(all_union)) / (n_users * k_suggestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions : \n",
      "http://dbpedia.org/resource/Alien_(film) > nope\n",
      "http://dbpedia.org/resource/Apocalypse_Now > nope\n",
      "http://dbpedia.org/resource/Beauty_and_the_Beast_(2017_film) > nope\n",
      "http://dbpedia.org/resource/Being_John_Malkovich > nope\n",
      "http://dbpedia.org/resource/Blade_Runner > nope\n",
      "http://dbpedia.org/resource/Braveheart_(1925_film) > nope\n",
      "http://dbpedia.org/resource/Gattaca > nope\n",
      "http://dbpedia.org/resource/Gladiator_(2000_film) > watched\n",
      "http://dbpedia.org/resource/Indiana_Jones_and_the_Last_Crusade > nope\n",
      "http://dbpedia.org/resource/Total_Recall_(2012_film) > nope\n",
      "\n",
      "\n",
      "Truth : \n",
      "http://dbpedia.org/resource/Aladdin_(1992_Golden_Films_film)\n",
      "http://dbpedia.org/resource/American_History_X\n",
      "http://dbpedia.org/resource/Armageddon_(1997_film)\n",
      "http://dbpedia.org/resource/Back_to_the_Future\n",
      "http://dbpedia.org/resource/Back_to_the_Future_Part_II\n",
      "http://dbpedia.org/resource/Back_to_the_Future_Part_III\n",
      "http://dbpedia.org/resource/Cast_Away\n",
      "http://dbpedia.org/resource/Dangerous_Liaisons\n",
      "http://dbpedia.org/resource/Dangerous_Minds\n",
      "http://dbpedia.org/resource/Dead_Poets_Society\n",
      "http://dbpedia.org/resource/E.T._the_Extra-Terrestrial\n",
      "http://dbpedia.org/resource/Edward_Scissorhands_(dance)\n",
      "http://dbpedia.org/resource/Everything_Is_Illuminated_(film)\n",
      "http://dbpedia.org/resource/Finding_Nemo\n",
      "http://dbpedia.org/resource/Free_Willy_(film_series)\n",
      "http://dbpedia.org/resource/Ghostbusters_(2016_film)\n",
      "http://dbpedia.org/resource/Gladiator_(2000_film)\n",
      "http://dbpedia.org/resource/Mary_Poppins_(film)\n",
      "http://dbpedia.org/resource/Memento_(film)\n",
      "http://dbpedia.org/resource/Minority_Report_(film)\n",
      "http://dbpedia.org/resource/Monty_Python's_Life_of_Brian\n",
      "http://dbpedia.org/resource/Monty_Python_and_the_Holy_Grail\n",
      "http://dbpedia.org/resource/Ocean's_Eleven\n",
      "http://dbpedia.org/resource/Once_Upon_a_Time_in_the_West\n",
      "http://dbpedia.org/resource/One_Hour_Photo\n",
      "http://dbpedia.org/resource/Pirates_of_the_Caribbean:_The_Curse_of_the_Black_Pearl\n",
      "http://dbpedia.org/resource/Sherlock_Holmes_(2010_film)\n",
      "http://dbpedia.org/resource/Shrek\n",
      "http://dbpedia.org/resource/Sleepers_(film)\n",
      "http://dbpedia.org/resource/Spider-Man_(1977_film)\n",
      "http://dbpedia.org/resource/Stigmata_(film)\n",
      "http://dbpedia.org/resource/Terminator_2:_Judgment_Day\n",
      "http://dbpedia.org/resource/Titanic_(1996_miniseries)\n",
      "http://dbpedia.org/resource/Watchmen_(film)\n",
      "http://dbpedia.org/resource/X-Men:_First_Class\n",
      "http://dbpedia.org/resource/X-Men_(film_series)\n"
     ]
    }
   ],
   "source": [
    "sample_user_idx = 0\n",
    "get_reasoning_paths([top_paths[sample_user_idx]], [truth_items[sample_user_idx]])\n",
    "compare_prediction_truth(top_items[sample_user_idx], truth_items[sample_user_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
