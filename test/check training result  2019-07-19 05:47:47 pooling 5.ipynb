{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Test KPRN Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"2019-07-19 05:47:47\"\n",
    "MODEL_DIR = \"../logs/{}\".format(TEST_CODE)\n",
    "CHOSEN_EPOCH = 5\n",
    "\n",
    "MAX_SEED_NUM = 3\n",
    "MAX_RELATION_NUM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = sorted(os.listdir(MODEL_DIR))\n",
    "choosen_weight = \"{}/{}\".format(MODEL_DIR, trained_weights[CHOSEN_EPOCH - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 08:49:16.321428 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 08:49:16.328650 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0801 08:49:16.366823 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0801 08:49:16.545424 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0801 08:49:16.561538 139658519906048 deprecation.py:506] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0801 08:49:16.984292 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0801 08:49:17.872109 139658519906048 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 08:49:17.884974 139658519906048 deprecation.py:323] From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(choosen_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ratings_re = open(\"../data/ratings_re.csv\").readlines()\n",
    "file_triples_idx = open(\"../data/triples_idx.txt\").readlines()\n",
    "file_moviesIdx = open(\"../data/moviesIdx.txt\").readlines()\n",
    "file_types = open(\"../data/types.txt\").readlines()\n",
    "file_entities = open(\"../data/entities.txt\").readlines()\n",
    "file_relations = open(\"../data/relations.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_entity_to_name():\n",
    "\n",
    "    entity_id_to_name = {}\n",
    "    for line in file_moviesIdx:\n",
    "        movie_title, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = movie_title\n",
    "\n",
    "    for line in file_entities:\n",
    "        entity_name, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = entity_name\n",
    "\n",
    "    return entity_id_to_name\n",
    "\n",
    "def _get_movie_title_to_entity_type():\n",
    "\n",
    "    movie_title_to_entity_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        movie_title_to_entity_type[entity] = entity_type\n",
    "\n",
    "    return movie_title_to_entity_type\n",
    "\n",
    "def _get_entity_list_with_type():\n",
    "\n",
    "    entity_list_with_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        if entity_type not in entity_list_with_type:\n",
    "            entity_list_with_type[entity_type] = []\n",
    "        entity_list_with_type[entity_type].append(entity)\n",
    "\n",
    "    return entity_list_with_type\n",
    "\n",
    "REL_ID_END = '200030'\n",
    "def _get_relation_to_name():\n",
    "\n",
    "    # Create relation id to name mapping\n",
    "    relation_id_to_name = {}\n",
    "    for line in file_relations:\n",
    "        relation_name, relation_id = line.strip().split()\n",
    "        relation_id = int(relation_id)\n",
    "        relation_id += 200000\n",
    "\n",
    "        # last 2 relation : spouse and relative has no inverse\n",
    "        if relation_id < 200023:\n",
    "            relation_id_to_name[str(relation_id + 1)] = relation_name + \"_inverse\"\n",
    "\n",
    "        relation_id_to_name[str(relation_id)] = relation_name\n",
    "\n",
    "    relation_id_to_name[REL_ID_END] = \"END\"\n",
    "    return relation_id_to_name\n",
    "\n",
    "def _get_entity_type_to_id():\n",
    "\n",
    "    list_of_type = [\n",
    "        \"Category\",\n",
    "        \"Company\",\n",
    "        \"Country\",\n",
    "        \"Genre\",\n",
    "        \"Movie\",\n",
    "        \"Person\",\n",
    "        \"User\",\n",
    "        \"#PAD_TOKEN\",\n",
    "        \"#UNK_ENTITY_TYPE\",\n",
    "    ]\n",
    "\n",
    "    entity_type_to_id = {list_of_type[i]: i for i in range(0, len(list_of_type))}\n",
    "    return entity_type_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_to_entity_type = _get_movie_title_to_entity_type()\n",
    "entity_list_with_type = _get_entity_list_with_type()\n",
    "entity_id_to_name = _get_entity_to_name()\n",
    "relation_id_to_name = _get_relation_to_name()\n",
    "entity_type_to_id = _get_entity_type_to_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ENTITY_ID_PADDING = 500000\n",
    "def get_type_from_entity_id(entity_id):\n",
    "    # user\n",
    "    if int(entity_id) > USER_ENTITY_ID_PADDING:\n",
    "        return \"User\"\n",
    "    elif entity_id_to_name[entity_id] in movie_title_to_entity_type:\n",
    "        return movie_title_to_entity_type[entity_id_to_name[entity_id]]\n",
    "    else:\n",
    "        return \"#UNK_ENTITY_TYPE\"\n",
    "    \n",
    "def path_to_string(paths):\n",
    "    string_paths = []\n",
    "    for path in paths:\n",
    "\n",
    "        entity_string = []\n",
    "        for entity in path.split():\n",
    "            entity_string.append(get_entity_name(str(entity)))\n",
    "        entity_string = \" -> \".join(entity_string)\n",
    "\n",
    "        string_paths.append(entity_string)\n",
    "\n",
    "    return string_paths\n",
    "\n",
    "def get_entity_name(entity_id):\n",
    "\n",
    "    if entity_id in entity_id_to_name:\n",
    "        return entity_id_to_name[entity_id]\n",
    "    elif entity_id in relation_id_to_name:\n",
    "        return relation_id_to_name[entity_id]\n",
    "    else:\n",
    "        return \"user_{}\".format(entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Load KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KG from cache\n",
    "kg_path = pickle.load(open(\"../data/cache_kg_path\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Prepare path for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRREGULAR_RELATION = ['200024', '200025']\n",
    "NUM_OF_ITEMS = len(file_moviesIdx)\n",
    "\n",
    "def _generate_path_from_entity_to_all_others(entity_id, keep_relation_ratio=0.5, max_relation_num=30, max_entity_per_relation=1):\n",
    "\n",
    "    generated_paths = []\n",
    "\n",
    "    # Get direct relation attached to entity1\n",
    "    r1 = kg_path[entity_id]\n",
    "\n",
    "    # List all possible item-entities\n",
    "    all_item_entities = [str(x) for x in range(0, NUM_OF_ITEMS)]\n",
    "    for e2_id in all_item_entities:\n",
    "\n",
    "        # Skip if e1 == e2 (path to itself)\n",
    "        if e2_id == entity_id:\n",
    "            continue\n",
    "\n",
    "        # Get relations attached to entity2\n",
    "        r2 = kg_path[e2_id]\n",
    "\n",
    "        # Downsample intersect relation\n",
    "        intersect_relations = list(set(r1.keys()).intersection(set(r2.keys())))\n",
    "        if len(intersect_relations) < max_relation_num:\n",
    "            n_intersect_relation = len(intersect_relations)\n",
    "        else:\n",
    "            n_intersect_relation = max(max_relation_num, int(len(intersect_relations) * keep_relation_ratio))\n",
    "\n",
    "        # Find intersect between relation 1 and relation 2\n",
    "        np.random.shuffle(intersect_relations)\n",
    "        for relation in intersect_relations[:n_intersect_relation]:\n",
    "\n",
    "            # Find entities attached to relation (non-item entities)\n",
    "            intersect_entity = list(set(r1[relation]).intersection(set(r2[relation])))\n",
    "\n",
    "            # Down sample intersecting entities\n",
    "            np.random.shuffle(intersect_entity)\n",
    "            for mid_entity in intersect_entity[:max_entity_per_relation]:\n",
    "\n",
    "                # Format path and add to result\n",
    "                inverse_relation = relation if (relation in IRREGULAR_RELATION) else str(int(relation) + 1)\n",
    "                path = \"{} {} {} {} {}\".format(entity_id, relation, mid_entity, inverse_relation, e2_id)\n",
    "                generated_paths.append(path)\n",
    "\n",
    "    return generated_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_ID_GIVEN_GOOD_RATING = '200027'\n",
    "def _generate_all_path_from_user(user_id, max_seed_num=10, max_seed_ratio=0.2):\n",
    "\n",
    "    # List down all user interacted items\n",
    "    user_interacted_items = []\n",
    "    for relation in kg_path[user_id]:\n",
    "        user_interacted_items += kg_path[user_id][relation]\n",
    "    user_interacted_items = sorted(set(user_interacted_items))\n",
    "\n",
    "    # Downsample seeds (item interacted)\n",
    "    if len(user_interacted_items) < max_seed_num:\n",
    "        n_seed = len(user_interacted_items)\n",
    "    else:\n",
    "        n_seed = max(max_seed_num, int(len(user_interacted_items) * max_seed_ratio))\n",
    "    np.random.shuffle(user_interacted_items)\n",
    "    user_interacted_items = user_interacted_items[:n_seed]\n",
    "\n",
    "    # ====== Threading ======\n",
    "    user_paths = []\n",
    "    threads = []\n",
    "    for i in range(0, n_seed):\n",
    "\n",
    "        # Split items id equally\n",
    "        thread_items = user_interacted_items[i::n_seed]\n",
    "        thread = threading.Thread(target=_run_thread, args=(i, thread_items, user_paths))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all thread to finish\n",
    "    for i in range(0, n_seed):\n",
    "        threads[i].join()\n",
    "\n",
    "    # Reformat : add user id and 'REL_ID_GIVEN_GOOD_RATING'\n",
    "    user_paths = [\"{} {} {}\".format(user_id, REL_ID_GIVEN_GOOD_RATING, x) for x in user_paths]\n",
    "    return user_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run path preparation using thread to reduce time\n",
    "def _run_thread(thread_id, thread_items, result):\n",
    "    for item_id in (thread_items):\n",
    "        result += _generate_path_from_entity_to_all_others(item_id, keep_relation_ratio=0.5,\n",
    "                                                           max_relation_num=MAX_RELATION_NUM, max_entity_per_relation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reformat_user_path(user_paths):\n",
    "\n",
    "    new_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for path in user_paths:\n",
    "        e1, r1, e2, r2, e3, r3, e4 = path.strip().split()\n",
    "\n",
    "        t1 = entity_type_to_id[get_type_from_entity_id(e1)]\n",
    "        t2 = entity_type_to_id[get_type_from_entity_id(e2)]\n",
    "        t3 = entity_type_to_id[get_type_from_entity_id(e3)]\n",
    "        t4 = entity_type_to_id[get_type_from_entity_id(e4)]\n",
    "\n",
    "        r4 = REL_ID_END\n",
    "\n",
    "        entity_rated = kg_path[e1][REL_ID_GIVEN_GOOD_RATING]\n",
    "        label = 1 if e4 in entity_rated else 0\n",
    "\n",
    "        new_paths.append([\n",
    "            [e1, t1, r1],\n",
    "            [e2, t2, r2],\n",
    "            [e3, t3, r3],\n",
    "            [e4, t4, r4],\n",
    "        ])\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(new_paths).astype('int'), np.array(labels).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=-1):\n",
    "    \"\"\"\n",
    "    max_pooling_size -1 means without pooling\n",
    "    \"\"\"\n",
    "    if max_pooling_size > 0:\n",
    "        return _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=max_pooling_size, get_best=get_best)\n",
    "    else:\n",
    "        return _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=get_best)\n",
    "\n",
    "def _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on y_pred scores, \n",
    "    pick all path in sorted X_test until the amount of unique items is equal to k\n",
    "    \"\"\"\n",
    "    choosen_paths = []\n",
    "    choosen_items = set()\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "        choosen_paths.append((path, score))\n",
    "        choosen_items.add(path[3][0])  # Add the last item\n",
    "\n",
    "        if len(choosen_items) >= k:\n",
    "            break\n",
    "\n",
    "    return choosen_paths, choosen_items\n",
    "\n",
    "def _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=5, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on averaged y_pred scores,\n",
    "    pick 'max_pooling_size' paths for each item, rank item based on average score, pick top k items.\n",
    "    \"\"\"\n",
    "    paths_group_by_items = {}\n",
    "    score_group_by_items = {}\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "\n",
    "        ending_item = path[3][0]  # The last item before END RELATION\n",
    "\n",
    "        if ending_item not in paths_group_by_items:\n",
    "            paths_group_by_items[ending_item] = []\n",
    "            score_group_by_items[ending_item] = []\n",
    "\n",
    "        # Pool the paths and score\n",
    "        if len(paths_group_by_items[ending_item]) < max_pooling_size:\n",
    "            paths_group_by_items[ending_item].append((path, score))\n",
    "            score_group_by_items[ending_item].append(score)\n",
    "\n",
    "    # Calculate average\n",
    "    for item in score_group_by_items:\n",
    "        score_group_by_items[item] = sum(score_group_by_items[item]) / len(score_group_by_items[item])\n",
    "\n",
    "    # Sort pooled result\n",
    "    sorted_average_score = sorted(score_group_by_items.items(), key=lambda kv: kv[1], reverse=get_best)\n",
    "    choosen_items = {key for key, v in sorted_average_score[:k]}\n",
    "\n",
    "    # Filter top-k items with highest average score\n",
    "    choosen_paths = []\n",
    "    for item_id in choosen_items:\n",
    "        choosen_paths += paths_group_by_items[item_id]\n",
    "\n",
    "    return choosen_paths, choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestion(user_id, k=10, max_pooling_size=-1):\n",
    "\n",
    "    user_paths = _generate_all_path_from_user(user_id, max_seed_num=MAX_SEED_NUM, max_seed_ratio=0.3)\n",
    "    X_test, _ = _reformat_user_path(user_paths)\n",
    "\n",
    "    y_pred = model.predict(X_test, batch_size=2048, verbose=1)\n",
    "\n",
    "    top_paths, top_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=max_pooling_size)\n",
    "    worst_path, worst_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=False, max_pooling_size=max_pooling_size)\n",
    "\n",
    "    return top_paths, top_choosen_items, worst_path, worst_choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user_id):\n",
    "    return set(kg_path[user_id][REL_ID_GIVEN_GOOD_RATING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(pred, truth, k=10):\n",
    "\n",
    "    # Make sure same type\n",
    "    pred = {str(x) for x in pred}\n",
    "    truth = {str(x) for x in truth}\n",
    "\n",
    "    intersect = pred.intersection(truth)\n",
    "\n",
    "    len_intersect = len(intersect)\n",
    "    len_truth = len(truth) if 0 < len(truth) <= k else k\n",
    "\n",
    "    return intersect, len_intersect / len_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasoning_paths(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reason = \" -> \".join(reason_path)\n",
    "            reasoning_paths.append(reason)\n",
    "    return reasoning_paths\n",
    "\n",
    "\n",
    "def get_reasoning_paths_df(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reasoning_paths.append(reason_path)\n",
    "    return pd.DataFrame(reasoning_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare_prediction_truth(predictions, truths):\n",
    "\n",
    "    predictions = [get_entity_name(str(p)) for p in predictions]\n",
    "    truths = [get_entity_name(str(t)) for t in truths]\n",
    "\n",
    "    print(\"Predictions : \")\n",
    "    for p in sorted(predictions):\n",
    "        is_watched = \"watched\" if str(p) in truths else \"nope\"\n",
    "        print(\"{} > {}\".format(p, is_watched))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Truth : \")\n",
    "    for t in sorted(truths):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291800/291800 [==============================] - 2s 5us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [02:47<25:04, 167.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339517/339517 [==============================] - 1s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [04:34<19:54, 149.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279358/279358 [==============================] - 1s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [07:20<17:59, 154.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93097/93097 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [08:23<12:41, 127.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203434/203434 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [10:55<11:11, 134.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86215/86215 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [11:44<07:14, 108.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342937/342937 [==============================] - 1s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [13:37<05:30, 110.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125646/125646 [==============================] - 0s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [14:23<03:01, 90.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67149/67149 [==============================] - 0s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [14:37<01:07, 67.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78865/78865 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [15:24<00:00, 61.56s/it]\n"
     ]
    }
   ],
   "source": [
    "k_suggestions = 10\n",
    "n_users = 10\n",
    "\n",
    "sample_user = np.random.randint(500001, 630000, n_users)\n",
    "sample_user = [str(x) for x in sample_user]\n",
    "\n",
    "top_paths = []\n",
    "top_items = []\n",
    "worst_paths = []\n",
    "worst_items = []\n",
    "\n",
    "truth_items = []\n",
    "\n",
    "n_paths = []\n",
    "intersects = []\n",
    "scores = []\n",
    "\n",
    "all_intersect = None\n",
    "all_union = None\n",
    "\n",
    "for user in tqdm(sample_user):\n",
    "    top_suggested_path, top_suggested_items, worst_suggested_path, worst_suggested_items = get_suggestion(user, k=k_suggestions, max_pooling_size=5)\n",
    "    top_truth_items = get_top_truth(user)\n",
    "\n",
    "    intersect, score = check_precision(top_suggested_items, top_truth_items)\n",
    "\n",
    "    top_paths.append(top_suggested_path)\n",
    "    top_items.append(top_suggested_items)\n",
    "\n",
    "    worst_paths.append(worst_suggested_path)\n",
    "    worst_items.append(worst_suggested_items)\n",
    "\n",
    "    intersects.append(intersect)\n",
    "    truth_items.append(top_truth_items)\n",
    "\n",
    "    n_paths.append(len(top_suggested_path))\n",
    "    scores.append(score)\n",
    "\n",
    "    if all_intersect is None:\n",
    "        all_intersect = top_suggested_items\n",
    "    else:\n",
    "        all_intersect = all_intersect.intersection(top_suggested_items)\n",
    "\n",
    "    if all_union is None:\n",
    "        all_union = top_suggested_items\n",
    "    else:\n",
    "        all_union = all_union.union(top_suggested_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec@k score: 0.185\n",
      "\n",
      "intersect\n",
      "{5058, 10659} 2\n",
      "\n",
      "union\n",
      "{8962, 7304, 911, 11536, 5013, 534, 11677, 546, 10659, 676, 10796, 5935, 5937, 5058, 13122, 14025, 3401, 3789, 4687, 7631, 14164, 5588, 5214, 8672, 3815, 4071, 1774, 13808, 5619, 9846, 4215, 5629} 32\n",
      "\n",
      "distinct rate\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "print(\"Prec@k score:\", np.average(scores))\n",
    "# print(\"top_suggested_items:\", top_suggested_items)\n",
    "# print(\"truth_items:\", truth_items)\n",
    "\n",
    "print(\"\\nintersect\")\n",
    "print(all_intersect, len(all_intersect))\n",
    "print(\"\\nunion\")\n",
    "print(all_union, len(all_union))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(all_union)) / (n_users * k_suggestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions : \n",
      "http://dbpedia.org/resource/Alien_(film) > nope\n",
      "http://dbpedia.org/resource/Apollo_13_(film) > nope\n",
      "http://dbpedia.org/resource/Blade_Runner > nope\n",
      "http://dbpedia.org/resource/Die_Hard > nope\n",
      "http://dbpedia.org/resource/Fargo_(film) > nope\n",
      "http://dbpedia.org/resource/Forrest_Gump > watched\n",
      "http://dbpedia.org/resource/Good_Will_Hunting > nope\n",
      "http://dbpedia.org/resource/Pulp_Fiction > watched\n",
      "http://dbpedia.org/resource/Schindler's_List > nope\n",
      "http://dbpedia.org/resource/Toy_Story > watched\n",
      "\n",
      "\n",
      "Truth : \n",
      "http://dbpedia.org/resource/300_(film)\n",
      "http://dbpedia.org/resource/Alice_in_Wonderland_(Disney_film)\n",
      "http://dbpedia.org/resource/American_History_X\n",
      "http://dbpedia.org/resource/American_Pie_(film_series)\n",
      "http://dbpedia.org/resource/Army_of_Darkness\n",
      "http://dbpedia.org/resource/Bubble_Boy_(film)\n",
      "http://dbpedia.org/resource/Edward_Scissorhands_(dance)\n",
      "http://dbpedia.org/resource/Fight_Club\n",
      "http://dbpedia.org/resource/Finding_Nemo\n",
      "http://dbpedia.org/resource/Forrest_Gump\n",
      "http://dbpedia.org/resource/Gladiator_(2000_film)\n",
      "http://dbpedia.org/resource/Gran_Torino\n",
      "http://dbpedia.org/resource/Hot_Fuzz\n",
      "http://dbpedia.org/resource/Little_Big_League\n",
      "http://dbpedia.org/resource/Meet_the_Parents_(1992_film)\n",
      "http://dbpedia.org/resource/Men_in_Black_(film_series)\n",
      "http://dbpedia.org/resource/Office_Space\n",
      "http://dbpedia.org/resource/Pirates_of_the_Caribbean:_Dead_Man's_Chest\n",
      "http://dbpedia.org/resource/Pirates_of_the_Caribbean:_The_Curse_of_the_Black_Pearl\n",
      "http://dbpedia.org/resource/Pulp_Fiction\n",
      "http://dbpedia.org/resource/Robin_Hood:_Men_in_Tights\n",
      "http://dbpedia.org/resource/Shaun_of_the_Dead\n",
      "http://dbpedia.org/resource/Sherlock_Holmes_(2010_film)\n",
      "http://dbpedia.org/resource/Slumdog_Millionaire\n",
      "http://dbpedia.org/resource/Snatch_(film)\n",
      "http://dbpedia.org/resource/Spider-Man_(1977_film)\n",
      "http://dbpedia.org/resource/Superbad_(film)\n",
      "http://dbpedia.org/resource/Terminator_3:_Rise_of_the_Machines\n",
      "http://dbpedia.org/resource/This_Is_Spinal_Tap\n",
      "http://dbpedia.org/resource/Toy_Story\n",
      "http://dbpedia.org/resource/Toy_Story_2\n",
      "http://dbpedia.org/resource/Transformers_(film_series)\n",
      "http://dbpedia.org/resource/Tropic_Thunder\n",
      "http://dbpedia.org/resource/V_for_Vendetta_(film)\n",
      "http://dbpedia.org/resource/Wedding_Crashers\n",
      "http://dbpedia.org/resource/X-Men_(film_series)\n",
      "http://dbpedia.org/resource/Zombieland\n"
     ]
    }
   ],
   "source": [
    "sample_user_idx = 0\n",
    "get_reasoning_paths([top_paths[sample_user_idx]], [truth_items[sample_user_idx]])\n",
    "compare_prediction_truth(top_items[sample_user_idx], truth_items[sample_user_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
