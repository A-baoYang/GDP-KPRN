{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Test KPRN Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"2019-07-17 12:51:28\"\n",
    "MODEL_DIR = \"../logs/{}\".format(TEST_CODE)\n",
    "CHOSEN_EPOCH = 5\n",
    "\n",
    "MAX_SEED_NUM = 3\n",
    "MAX_RELATION_NUM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = sorted(os.listdir(MODEL_DIR))\n",
    "choosen_weight = \"{}/{}\".format(MODEL_DIR, trained_weights[CHOSEN_EPOCH - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 08:47:05.955432 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0801 08:47:05.960831 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0801 08:47:05.984531 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0801 08:47:06.094266 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0801 08:47:06.105398 139668383332096 deprecation.py:506] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0801 08:47:06.947363 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0801 08:47:07.698915 139668383332096 deprecation_wrapper.py:119] From /home/jessinra/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0801 08:47:07.709745 139668383332096 deprecation.py:323] From /home/jessinra/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(choosen_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ratings_re = open(\"../data/ratings_re.csv\").readlines()\n",
    "file_triples_idx = open(\"../data/triples_idx.txt\").readlines()\n",
    "file_moviesIdx = open(\"../data/moviesIdx.txt\").readlines()\n",
    "file_types = open(\"../data/types.txt\").readlines()\n",
    "file_entities = open(\"../data/entities.txt\").readlines()\n",
    "file_relations = open(\"../data/relations.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_entity_to_name():\n",
    "\n",
    "    entity_id_to_name = {}\n",
    "    for line in file_moviesIdx:\n",
    "        movie_title, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = movie_title\n",
    "\n",
    "    for line in file_entities:\n",
    "        entity_name, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = entity_name\n",
    "\n",
    "    return entity_id_to_name\n",
    "\n",
    "def _get_movie_title_to_entity_type():\n",
    "\n",
    "    movie_title_to_entity_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        movie_title_to_entity_type[entity] = entity_type\n",
    "\n",
    "    return movie_title_to_entity_type\n",
    "\n",
    "def _get_entity_list_with_type():\n",
    "\n",
    "    entity_list_with_type = {}\n",
    "    for line in file_types:\n",
    "\n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        if entity_type not in entity_list_with_type:\n",
    "            entity_list_with_type[entity_type] = []\n",
    "        entity_list_with_type[entity_type].append(entity)\n",
    "\n",
    "    return entity_list_with_type\n",
    "\n",
    "REL_ID_END = '200030'\n",
    "def _get_relation_to_name():\n",
    "\n",
    "    # Create relation id to name mapping\n",
    "    relation_id_to_name = {}\n",
    "    for line in file_relations:\n",
    "        relation_name, relation_id = line.strip().split()\n",
    "        relation_id = int(relation_id)\n",
    "        relation_id += 200000\n",
    "\n",
    "        # last 2 relation : spouse and relative has no inverse\n",
    "        if relation_id < 200023:\n",
    "            relation_id_to_name[str(relation_id + 1)] = relation_name + \"_inverse\"\n",
    "\n",
    "        relation_id_to_name[str(relation_id)] = relation_name\n",
    "\n",
    "    relation_id_to_name[REL_ID_END] = \"END\"\n",
    "    return relation_id_to_name\n",
    "\n",
    "def _get_entity_type_to_id():\n",
    "\n",
    "    list_of_type = [\n",
    "        \"Category\",\n",
    "        \"Company\",\n",
    "        \"Country\",\n",
    "        \"Genre\",\n",
    "        \"Movie\",\n",
    "        \"Person\",\n",
    "        \"User\",\n",
    "        \"#PAD_TOKEN\",\n",
    "        \"#UNK_ENTITY_TYPE\",\n",
    "    ]\n",
    "\n",
    "    entity_type_to_id = {list_of_type[i]: i for i in range(0, len(list_of_type))}\n",
    "    return entity_type_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_to_entity_type = _get_movie_title_to_entity_type()\n",
    "entity_list_with_type = _get_entity_list_with_type()\n",
    "entity_id_to_name = _get_entity_to_name()\n",
    "relation_id_to_name = _get_relation_to_name()\n",
    "entity_type_to_id = _get_entity_type_to_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ENTITY_ID_PADDING = 500000\n",
    "def get_type_from_entity_id(entity_id):\n",
    "    # user\n",
    "    if int(entity_id) > USER_ENTITY_ID_PADDING:\n",
    "        return \"User\"\n",
    "    elif entity_id_to_name[entity_id] in movie_title_to_entity_type:\n",
    "        return movie_title_to_entity_type[entity_id_to_name[entity_id]]\n",
    "    else:\n",
    "        return \"#UNK_ENTITY_TYPE\"\n",
    "    \n",
    "def path_to_string(paths):\n",
    "    string_paths = []\n",
    "    for path in paths:\n",
    "\n",
    "        entity_string = []\n",
    "        for entity in path.split():\n",
    "            entity_string.append(get_entity_name(str(entity)))\n",
    "        entity_string = \" -> \".join(entity_string)\n",
    "\n",
    "        string_paths.append(entity_string)\n",
    "\n",
    "    return string_paths\n",
    "\n",
    "def get_entity_name(entity_id):\n",
    "\n",
    "    if entity_id in entity_id_to_name:\n",
    "        return entity_id_to_name[entity_id]\n",
    "    elif entity_id in relation_id_to_name:\n",
    "        return relation_id_to_name[entity_id]\n",
    "    else:\n",
    "        return \"user_{}\".format(entity_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Load KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KG from cache\n",
    "kg_path = pickle.load(open(\"../data/cache_kg_path\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Prepare path for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRREGULAR_RELATION = ['200024', '200025']\n",
    "NUM_OF_ITEMS = len(file_moviesIdx)\n",
    "\n",
    "def _generate_path_from_entity_to_all_others(entity_id, keep_relation_ratio=0.5, max_relation_num=30, max_entity_per_relation=1):\n",
    "\n",
    "    generated_paths = []\n",
    "\n",
    "    # Get direct relation attached to entity1\n",
    "    r1 = kg_path[entity_id]\n",
    "\n",
    "    # List all possible item-entities\n",
    "    all_item_entities = [str(x) for x in range(0, NUM_OF_ITEMS)]\n",
    "    for e2_id in all_item_entities:\n",
    "\n",
    "        # Skip if e1 == e2 (path to itself)\n",
    "        if e2_id == entity_id:\n",
    "            continue\n",
    "\n",
    "        # Get relations attached to entity2\n",
    "        r2 = kg_path[e2_id]\n",
    "\n",
    "        # Downsample intersect relation\n",
    "        intersect_relations = list(set(r1.keys()).intersection(set(r2.keys())))\n",
    "        if len(intersect_relations) < max_relation_num:\n",
    "            n_intersect_relation = len(intersect_relations)\n",
    "        else:\n",
    "            n_intersect_relation = max(max_relation_num, int(len(intersect_relations) * keep_relation_ratio))\n",
    "\n",
    "        # Find intersect between relation 1 and relation 2\n",
    "        np.random.shuffle(intersect_relations)\n",
    "        for relation in intersect_relations[:n_intersect_relation]:\n",
    "\n",
    "            # Find entities attached to relation (non-item entities)\n",
    "            intersect_entity = list(set(r1[relation]).intersection(set(r2[relation])))\n",
    "\n",
    "            # Down sample intersecting entities\n",
    "            np.random.shuffle(intersect_entity)\n",
    "            for mid_entity in intersect_entity[:max_entity_per_relation]:\n",
    "\n",
    "                # Format path and add to result\n",
    "                inverse_relation = relation if (relation in IRREGULAR_RELATION) else str(int(relation) + 1)\n",
    "                path = \"{} {} {} {} {}\".format(entity_id, relation, mid_entity, inverse_relation, e2_id)\n",
    "                generated_paths.append(path)\n",
    "\n",
    "    return generated_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_ID_GIVEN_GOOD_RATING = '200027'\n",
    "def _generate_all_path_from_user(user_id, max_seed_num=10, max_seed_ratio=0.2):\n",
    "\n",
    "    # List down all user interacted items\n",
    "    user_interacted_items = []\n",
    "    for relation in kg_path[user_id]:\n",
    "        user_interacted_items += kg_path[user_id][relation]\n",
    "    user_interacted_items = sorted(set(user_interacted_items))\n",
    "\n",
    "    # Downsample seeds (item interacted)\n",
    "    if len(user_interacted_items) < max_seed_num:\n",
    "        n_seed = len(user_interacted_items)\n",
    "    else:\n",
    "        n_seed = max(max_seed_num, int(len(user_interacted_items) * max_seed_ratio))\n",
    "    np.random.shuffle(user_interacted_items)\n",
    "    user_interacted_items = user_interacted_items[:n_seed]\n",
    "\n",
    "    # ====== Threading ======\n",
    "    user_paths = []\n",
    "    threads = []\n",
    "    for i in range(0, n_seed):\n",
    "\n",
    "        # Split items id equally\n",
    "        thread_items = user_interacted_items[i::n_seed]\n",
    "        thread = threading.Thread(target=_run_thread, args=(i, thread_items, user_paths))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all thread to finish\n",
    "    for i in range(0, n_seed):\n",
    "        threads[i].join()\n",
    "\n",
    "    # Reformat : add user id and 'REL_ID_GIVEN_GOOD_RATING'\n",
    "    user_paths = [\"{} {} {}\".format(user_id, REL_ID_GIVEN_GOOD_RATING, x) for x in user_paths]\n",
    "    return user_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run path preparation using thread to reduce time\n",
    "def _run_thread(thread_id, thread_items, result):\n",
    "    for item_id in (thread_items):\n",
    "        result += _generate_path_from_entity_to_all_others(item_id, keep_relation_ratio=0.5,\n",
    "                                                           max_relation_num=MAX_RELATION_NUM, max_entity_per_relation=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reformat_user_path(user_paths):\n",
    "\n",
    "    new_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for path in user_paths:\n",
    "        e1, r1, e2, r2, e3, r3, e4 = path.strip().split()\n",
    "\n",
    "        t1 = entity_type_to_id[get_type_from_entity_id(e1)]\n",
    "        t2 = entity_type_to_id[get_type_from_entity_id(e2)]\n",
    "        t3 = entity_type_to_id[get_type_from_entity_id(e3)]\n",
    "        t4 = entity_type_to_id[get_type_from_entity_id(e4)]\n",
    "\n",
    "        r4 = REL_ID_END\n",
    "\n",
    "        entity_rated = kg_path[e1][REL_ID_GIVEN_GOOD_RATING]\n",
    "        label = 1 if e4 in entity_rated else 0\n",
    "\n",
    "        new_paths.append([\n",
    "            [e1, t1, r1],\n",
    "            [e2, t2, r2],\n",
    "            [e3, t3, r3],\n",
    "            [e4, t4, r4],\n",
    "        ])\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(new_paths).astype('int'), np.array(labels).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=-1):\n",
    "    \"\"\"\n",
    "    max_pooling_size -1 means without pooling\n",
    "    \"\"\"\n",
    "    if max_pooling_size > 0:\n",
    "        return _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=max_pooling_size, get_best=get_best)\n",
    "    else:\n",
    "        return _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=get_best)\n",
    "\n",
    "def _get_top_k_items_without_score_pooling(X_test, y_pred, k=10, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on y_pred scores, \n",
    "    pick all path in sorted X_test until the amount of unique items is equal to k\n",
    "    \"\"\"\n",
    "    choosen_paths = []\n",
    "    choosen_items = set()\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "        choosen_paths.append((path, score))\n",
    "        choosen_items.add(path[3][0])  # Add the last item\n",
    "\n",
    "        if len(choosen_items) >= k:\n",
    "            break\n",
    "\n",
    "    return choosen_paths, choosen_items\n",
    "\n",
    "def _get_top_k_items_with_score_pooling(X_test, y_pred, k=10, max_pooling_size=5, get_best=True):\n",
    "    \"\"\"\n",
    "    Get the top-k items from X_test based on averaged y_pred scores,\n",
    "    pick 'max_pooling_size' paths for each item, rank item based on average score, pick top k items.\n",
    "    \"\"\"\n",
    "    paths_group_by_items = {}\n",
    "    score_group_by_items = {}\n",
    "\n",
    "    path_scores = sorted(zip(X_test, y_pred), key=lambda x: x[1], reverse=get_best)\n",
    "    for path, score in path_scores:\n",
    "\n",
    "        ending_item = path[3][0]  # The last item before END RELATION\n",
    "\n",
    "        if ending_item not in paths_group_by_items:\n",
    "            paths_group_by_items[ending_item] = []\n",
    "            score_group_by_items[ending_item] = []\n",
    "\n",
    "        # Pool the paths and score\n",
    "        if len(paths_group_by_items[ending_item]) < max_pooling_size:\n",
    "            paths_group_by_items[ending_item].append((path, score))\n",
    "            score_group_by_items[ending_item].append(score)\n",
    "\n",
    "    # Calculate average\n",
    "    for item in score_group_by_items:\n",
    "        score_group_by_items[item] = sum(score_group_by_items[item]) / len(score_group_by_items[item])\n",
    "\n",
    "    # Sort pooled result\n",
    "    sorted_average_score = sorted(score_group_by_items.items(), key=lambda kv: kv[1], reverse=get_best)\n",
    "    choosen_items = {key for key, v in sorted_average_score[:k]}\n",
    "\n",
    "    # Filter top-k items with highest average score\n",
    "    choosen_paths = []\n",
    "    for item_id in choosen_items:\n",
    "        choosen_paths += paths_group_by_items[item_id]\n",
    "\n",
    "    return choosen_paths, choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestion(user_id, k=10, max_pooling_size=-1):\n",
    "\n",
    "    user_paths = _generate_all_path_from_user(user_id, max_seed_num=MAX_SEED_NUM, max_seed_ratio=0.3)\n",
    "    X_test, _ = _reformat_user_path(user_paths)\n",
    "\n",
    "    y_pred = model.predict(X_test, batch_size=2048, verbose=1)\n",
    "\n",
    "    top_paths, top_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=True, max_pooling_size=max_pooling_size)\n",
    "    worst_path, worst_choosen_items = _get_k_prediction(X_test, y_pred, k=10, get_best=False, max_pooling_size=max_pooling_size)\n",
    "\n",
    "    return top_paths, top_choosen_items, worst_path, worst_choosen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_truth(user_id):\n",
    "    return set(kg_path[user_id][REL_ID_GIVEN_GOOD_RATING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(pred, truth, k=10):\n",
    "\n",
    "    # Make sure same type\n",
    "    pred = {str(x) for x in pred}\n",
    "    truth = {str(x) for x in truth}\n",
    "\n",
    "    intersect = pred.intersection(truth)\n",
    "\n",
    "    len_intersect = len(intersect)\n",
    "    len_truth = len(truth) if 0 < len(truth) <= k else k\n",
    "\n",
    "    return intersect, len_intersect / len_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reasoning_paths(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reason = \" -> \".join(reason_path)\n",
    "            reasoning_paths.append(reason)\n",
    "    return reasoning_paths\n",
    "\n",
    "\n",
    "def get_reasoning_paths_df(paths, truth_items=None):\n",
    "    reasoning_paths = []\n",
    "    for i in range(0, len(paths)):\n",
    "        user_path = paths[i]\n",
    "        user_truth = truth_items[i]\n",
    "\n",
    "        for path in user_path:\n",
    "\n",
    "            score = path[1]\n",
    "            reason_path = [str(score)]\n",
    "\n",
    "            for sequence in path[0]:\n",
    "                entity, _, relation = sequence\n",
    "                e = get_entity_name(str(entity))\n",
    "                r = get_entity_name(str(relation))\n",
    "\n",
    "                reason_path.append(e)\n",
    "                reason_path.append(r)\n",
    "\n",
    "                if str(relation) == REL_ID_END:\n",
    "                    watched = 'watched' if str(entity) in user_truth else \"nope\"\n",
    "                    reason_path.append(watched)\n",
    "\n",
    "            reasoning_paths.append(reason_path)\n",
    "    return pd.DataFrame(reasoning_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compare_prediction_truth(predictions, truths):\n",
    "\n",
    "    predictions = [get_entity_name(str(p)) for p in predictions]\n",
    "    truths = [get_entity_name(str(t)) for t in truths]\n",
    "\n",
    "    print(\"Predictions : \")\n",
    "    for p in sorted(predictions):\n",
    "        is_watched = \"watched\" if str(p) in truths else \"nope\"\n",
    "        print(\"{} > {}\".format(p, is_watched))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Truth : \")\n",
    "    for t in sorted(truths):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645972/645972 [==============================] - 3s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [04:23<39:29, 263.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149964/149964 [==============================] - 1s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [05:10<26:26, 198.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473520/473520 [==============================] - 2s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [11:17<29:03, 249.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440293/440293 [==============================] - 2s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [14:19<22:52, 228.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182403/182403 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [15:27<15:03, 180.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201171/201171 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [18:14<11:46, 176.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380900/380900 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [20:07<07:52, 157.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182484/182484 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [21:56<04:45, 142.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759984/759984 [==============================] - 2s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [26:38<03:04, 184.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172563/172563 [==============================] - 1s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [27:34<00:00, 146.07s/it]\n"
     ]
    }
   ],
   "source": [
    "k_suggestions = 10\n",
    "n_users = 10\n",
    "\n",
    "sample_user = np.random.randint(500001, 630000, n_users)\n",
    "sample_user = [str(x) for x in sample_user]\n",
    "\n",
    "top_paths = []\n",
    "top_items = []\n",
    "worst_paths = []\n",
    "worst_items = []\n",
    "\n",
    "truth_items = []\n",
    "\n",
    "n_paths = []\n",
    "intersects = []\n",
    "scores = []\n",
    "\n",
    "all_intersect = None\n",
    "all_union = None\n",
    "\n",
    "for user in tqdm(sample_user):\n",
    "    top_suggested_path, top_suggested_items, worst_suggested_path, worst_suggested_items = get_suggestion(user, k=k_suggestions, max_pooling_size=-1)\n",
    "    top_truth_items = get_top_truth(user)\n",
    "\n",
    "    intersect, score = check_precision(top_suggested_items, top_truth_items)\n",
    "\n",
    "    top_paths.append(top_suggested_path)\n",
    "    top_items.append(top_suggested_items)\n",
    "\n",
    "    worst_paths.append(worst_suggested_path)\n",
    "    worst_items.append(worst_suggested_items)\n",
    "\n",
    "    intersects.append(intersect)\n",
    "    truth_items.append(top_truth_items)\n",
    "\n",
    "    n_paths.append(len(top_suggested_path))\n",
    "    scores.append(score)\n",
    "\n",
    "    if all_intersect is None:\n",
    "        all_intersect = top_suggested_items\n",
    "    else:\n",
    "        all_intersect = all_intersect.intersection(top_suggested_items)\n",
    "\n",
    "    if all_union is None:\n",
    "        all_union = top_suggested_items\n",
    "    else:\n",
    "        all_union = all_union.union(top_suggested_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec@k score: 0.31\n",
      "\n",
      "intersect\n",
      "set() 0\n",
      "\n",
      "union\n",
      "{8961, 774, 6918, 11536, 534, 7703, 2073, 14365, 12072, 1322, 10796, 13872, 10545, 11314, 312, 5441, 13122, 13123, 3401, 4687, 3153, 9555, 854, 90, 7010, 1124, 12143, 12402, 11892, 8053, 9846, 15234, 10371, 15235, 14471, 7304, 652, 909, 3214, 911, 10644, 1172, 9620, 11677, 12451, 10659, 676, 11690, 4274, 6834, 3252, 6835, 4791, 9402, 5058, 14025, 10187, 11469, 7631, 2004, 477, 5086, 8672, 15075, 996, 11494, 8166, 3816, 491, 11757, 8688, 12787, 5619, 12790, 11006} 75\n",
      "\n",
      "distinct rate\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Prec@k score:\", np.average(scores))\n",
    "# print(\"top_suggested_items:\", top_suggested_items)\n",
    "# print(\"truth_items:\", truth_items)\n",
    "\n",
    "print(\"\\nintersect\")\n",
    "print(all_intersect, len(all_intersect))\n",
    "print(\"\\nunion\")\n",
    "print(all_union, len(all_union))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(all_union)) / (n_users * k_suggestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions : \n",
      "http://dbpedia.org/resource/Backbeat_(film) > nope\n",
      "http://dbpedia.org/resource/Born_Yesterday_(1950_film) > nope\n",
      "http://dbpedia.org/resource/Dances_with_Wolves > watched\n",
      "http://dbpedia.org/resource/It's_a_Wonderful_Life > watched\n",
      "http://dbpedia.org/resource/North_by_Northwest > watched\n",
      "http://dbpedia.org/resource/One_Flew_Over_the_Cuckoo's_Nest_(film) > nope\n",
      "http://dbpedia.org/resource/Psycho_(franchise) > watched\n",
      "http://dbpedia.org/resource/Saturday_Night_and_Sunday_Morning_(film) > nope\n",
      "http://dbpedia.org/resource/Seven_Samurai > nope\n",
      "http://dbpedia.org/resource/Unforgiven > watched\n",
      "\n",
      "\n",
      "Truth : \n",
      "http://dbpedia.org/resource/12_Angry_Men_(1957_film)\n",
      "http://dbpedia.org/resource/Alien_(film)\n",
      "http://dbpedia.org/resource/All_About_Eve\n",
      "http://dbpedia.org/resource/Amadeus_(film)\n",
      "http://dbpedia.org/resource/Annie_Hall\n",
      "http://dbpedia.org/resource/Arsenic_and_Old_Lace_(film)\n",
      "http://dbpedia.org/resource/As_Good_as_It_Gets\n",
      "http://dbpedia.org/resource/Back_to_the_Future_Part_III\n",
      "http://dbpedia.org/resource/Big_Night\n",
      "http://dbpedia.org/resource/Bound_(2015_film)\n",
      "http://dbpedia.org/resource/Bringing_Up_Baby\n",
      "http://dbpedia.org/resource/Butch_Cassidy_and_the_Sundance_Kid\n",
      "http://dbpedia.org/resource/Cape_Fear_(1991_film)\n",
      "http://dbpedia.org/resource/Casablanca_(film)\n",
      "http://dbpedia.org/resource/Casino_(film)\n",
      "http://dbpedia.org/resource/Citizen_Kane\n",
      "http://dbpedia.org/resource/Cliffhanger_(film)\n",
      "http://dbpedia.org/resource/Crumb_(film)\n",
      "http://dbpedia.org/resource/Dances_with_Wolves\n",
      "http://dbpedia.org/resource/Dave_(film)\n",
      "http://dbpedia.org/resource/Dial_M_for_Murder\n",
      "http://dbpedia.org/resource/Die_Hard\n",
      "http://dbpedia.org/resource/Dolores_Claiborne_(film)\n",
      "http://dbpedia.org/resource/Donnie_Brasco_(film)\n",
      "http://dbpedia.org/resource/E.T._the_Extra-Terrestrial\n",
      "http://dbpedia.org/resource/Fargo_(film)\n",
      "http://dbpedia.org/resource/Field_of_Dreams\n",
      "http://dbpedia.org/resource/Forrest_Gump\n",
      "http://dbpedia.org/resource/Four_Weddings_and_a_Funeral\n",
      "http://dbpedia.org/resource/French_Kiss_(2015_film)\n",
      "http://dbpedia.org/resource/From_Here_to_Eternity\n",
      "http://dbpedia.org/resource/Giant_(2009_film)\n",
      "http://dbpedia.org/resource/Glengarry_Glen_Ross_(film)\n",
      "http://dbpedia.org/resource/Going_My_Way\n",
      "http://dbpedia.org/resource/Goodfellas\n",
      "http://dbpedia.org/resource/Halloween_(2007_film)\n",
      "http://dbpedia.org/resource/Hear_My_Song\n",
      "http://dbpedia.org/resource/High_Noon_(2013_Short_Film)\n",
      "http://dbpedia.org/resource/His_Girl_Friday\n",
      "http://dbpedia.org/resource/Home_Alone_(franchise)\n",
      "http://dbpedia.org/resource/How_Green_Was_My_Valley_(film)\n",
      "http://dbpedia.org/resource/In_the_Line_of_Fire\n",
      "http://dbpedia.org/resource/It's_a_Wonderful_Life\n",
      "http://dbpedia.org/resource/Jaws_(franchise)\n",
      "http://dbpedia.org/resource/Jerry_Maguire\n",
      "http://dbpedia.org/resource/L.A._Confidential_(film)\n",
      "http://dbpedia.org/resource/Laura_(1944_film)\n",
      "http://dbpedia.org/resource/Lethal_Weapon_(franchise)\n",
      "http://dbpedia.org/resource/Liar_Liar\n",
      "http://dbpedia.org/resource/Lone_Star_(1996_film)\n",
      "http://dbpedia.org/resource/Manhattan_(film)\n",
      "http://dbpedia.org/resource/Marty_(film)\n",
      "http://dbpedia.org/resource/Midnight_Cowboy\n",
      "http://dbpedia.org/resource/My_Family_(film)\n",
      "http://dbpedia.org/resource/My_Favorite_Year\n",
      "http://dbpedia.org/resource/Nobody's_Fool_(1986_film)\n",
      "http://dbpedia.org/resource/North_by_Northwest\n",
      "http://dbpedia.org/resource/On_Golden_Pond_(2001_film)\n",
      "http://dbpedia.org/resource/On_the_Waterfront\n",
      "http://dbpedia.org/resource/One_Fine_Day_(film)\n",
      "http://dbpedia.org/resource/Ordinary_People_(2009_film)\n",
      "http://dbpedia.org/resource/Paths_of_Glory\n",
      "http://dbpedia.org/resource/Patton_(film)\n",
      "http://dbpedia.org/resource/Picnic_(1955_film)\n",
      "http://dbpedia.org/resource/Psycho_(franchise)\n",
      "http://dbpedia.org/resource/Pulp_Fiction\n",
      "http://dbpedia.org/resource/Quiz_Show_(film)\n",
      "http://dbpedia.org/resource/Raging_Bull\n",
      "http://dbpedia.org/resource/Rear_Window_(1998_film)\n",
      "http://dbpedia.org/resource/Rebel_Without_a_Cause\n",
      "http://dbpedia.org/resource/Reservoir_Dogs\n",
      "http://dbpedia.org/resource/Rob_Roy_(1995_film)\n",
      "http://dbpedia.org/resource/Rocky_(film_series)\n",
      "http://dbpedia.org/resource/Schindler's_List\n",
      "http://dbpedia.org/resource/Searching_for_Bobby_Fischer\n",
      "http://dbpedia.org/resource/Secrets_&_Lies_(film)\n",
      "http://dbpedia.org/resource/Sense_and_Sensibility_(film)\n",
      "http://dbpedia.org/resource/Singin'_in_the_Rain\n",
      "http://dbpedia.org/resource/Sleepless_in_Seattle\n",
      "http://dbpedia.org/resource/Speed_(serial)\n",
      "http://dbpedia.org/resource/Taxi_Driver_(1981_film)\n",
      "http://dbpedia.org/resource/Terms_of_Endearment\n",
      "http://dbpedia.org/resource/To_Kill_a_Mockingbird_(film)\n",
      "http://dbpedia.org/resource/Top_Hat\n",
      "http://dbpedia.org/resource/Twister_(1996_film)\n",
      "http://dbpedia.org/resource/Unforgiven\n",
      "http://dbpedia.org/resource/Vertigo_(1951_film)\n",
      "http://dbpedia.org/resource/When_Harry_Met_Sally...\n",
      "http://dbpedia.org/resource/Wolf_(2013_film)\n"
     ]
    }
   ],
   "source": [
    "sample_user_idx = 0\n",
    "get_reasoning_paths([top_paths[sample_user_idx]], [truth_items[sample_user_idx]])\n",
    "compare_prediction_truth(top_items[sample_user_idx], truth_items[sample_user_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
