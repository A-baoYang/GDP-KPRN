{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><i> Preprocessing for KPRN </i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "from numpy.random import random_sample\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ratings_re = open(\"ratings_re.csv\").readlines()\n",
    "file_triples_idx = open(\"triples_idx.txt\").readlines()\n",
    "file_moviesIdx = open(\"moviesIdx.txt\").readlines() \n",
    "file_types = open(\"types.txt\").readlines() \n",
    "file_entities = open(\"entities.txt\").readlines()\n",
    "file_relations = open(\"relations.txt\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Prepare vocab generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_entity_to_name():\n",
    "\n",
    "    entity_id_to_name = {}\n",
    "    for line in file_moviesIdx:\n",
    "        movie_title, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = movie_title\n",
    "\n",
    "    for line in file_entities:\n",
    "        entity_name, entity_id = line.strip().split()\n",
    "        entity_id_to_name[entity_id] = entity_name\n",
    "        \n",
    "    return entity_id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_movie_title_to_entity_type():\n",
    "\n",
    "    movie_title_to_entity_type = {}\n",
    "    for line in file_types:\n",
    "        \n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        movie_title_to_entity_type[entity] = entity_type\n",
    "        \n",
    "    return movie_title_to_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_entity_list_with_type():\n",
    "\n",
    "    entity_list_with_type = {}\n",
    "    for line in file_types:\n",
    "        \n",
    "        entity, entity_type = line.strip().split('\\t')\n",
    "        if entity_type not in entity_list_with_type:\n",
    "            entity_list_with_type[entity_type] = []\n",
    "        entity_list_with_type[entity_type].append(entity)\n",
    "        \n",
    "    return entity_list_with_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relation_to_name():\n",
    "\n",
    "    # Create relation id to name mapping\n",
    "    relation_id_to_name = {}\n",
    "    for line in file_relations:\n",
    "        relation_name, relation_id = line.strip().split()\n",
    "        relation_id = int(relation_id)\n",
    "        relation_id += 200000\n",
    "\n",
    "        # last 2 relation : spouse and relative has no inverse\n",
    "        if relation_id < 200023:\n",
    "            relation_id_to_name[str(relation_id + 1)] = relation_name + \"_inverse\"\n",
    "\n",
    "        relation_id_to_name[str(relation_id)] = relation_name\n",
    "        \n",
    "    return relation_id_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run those script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title_to_entity_type = _get_movie_title_to_entity_type()\n",
    "entity_list_with_type = _get_entity_list_with_type()\n",
    "entity_id_to_name = _get_entity_to_name()\n",
    "relation_id_to_name = _get_relation_to_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minor value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Prepare KG path functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ENTITY_ID_PADDING = 500000\n",
    "REL_ID_RATED_GOOD_BY = '200026'\n",
    "REL_ID_GIVEN_GOOD_RATING = '200027'\n",
    "RATING_THRESHOLD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_kg_path_on_entity(kg_path):\n",
    "    \n",
    "    for line in tqdm(file_triples_idx):\n",
    "\n",
    "        head, relation, tail = line.strip().split()\n",
    "        if head not in kg_path:\n",
    "            kg_path[head] = {}\n",
    "\n",
    "        if relation not in kg_path[head]:\n",
    "            kg_path[head][relation] = []\n",
    "\n",
    "        kg_path[head][relation].append(tail)\n",
    "        \n",
    "    return kg_path\n",
    "\n",
    "def _generate_kg_path_on_collaborative_filtering(kg_path):\n",
    "    \n",
    "    for line in tqdm(file_ratings_re):\n",
    "        \n",
    "        user_id, movie_id, rating = line.split(',')[:3]\n",
    "        user_entity_id = str(USER_ENTITY_ID_PADDING + int(user_id))\n",
    "\n",
    "        # skip if movie is bad\n",
    "        if float(rating) < RATING_THRESHOLD :\n",
    "            continue\n",
    "\n",
    "        # Add given-good-rating list for each user\n",
    "        if user_entity_id not in kg_path:\n",
    "            kg_path[user_entity_id] = {}\n",
    "            kg_path[user_entity_id][REL_ID_GIVEN_GOOD_RATING] = []\n",
    "\n",
    "        # Add rated-good-by rating for the movie\n",
    "        if REL_ID_RATED_GOOD_BY not in kg_path[movie_id]:\n",
    "            kg_path[movie_id][REL_ID_RATED_GOOD_BY] = []\n",
    "\n",
    "        kg_path[user_entity_id][REL_ID_GIVEN_GOOD_RATING].append(movie_id)\n",
    "        kg_path[movie_id][REL_ID_RATED_GOOD_BY].append(user_entity_id)\n",
    "\n",
    "    return kg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_KG_FILENAME = \"cache_kg_path\"\n",
    "\n",
    "def create_kg_path():\n",
    "    \n",
    "    kg_path = dict()\n",
    "    kg_path = _generate_kg_path_on_entity(kg_path)\n",
    "    kg_path = _generate_kg_path_on_collaborative_filtering(kg_path)\n",
    "    \n",
    "    pickle.dump(kg_path, open(CACHE_KG_FILENAME, \"wb\"))\n",
    "    return kg_path\n",
    "\n",
    "def load_kg_path():\n",
    "    \n",
    "    try:\n",
    "        kg_path = pickle.load(open(CACHE_KG_FILENAME, \"rb\"))\n",
    "        \n",
    "    except:\n",
    "        kg_path = create_kg_path()\n",
    "        \n",
    "    finally:\n",
    "        return kg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_path = load_kg_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(list_obj, filename):\n",
    "    with open(filename, 'w') as writer:\n",
    "        for item in list_obj:\n",
    "            if \"\\n\" in item:\n",
    "                writer.write(item)\n",
    "            else:\n",
    "                writer.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_user_rated_potential_movie(user_id, potential_id):\n",
    "    return potential_id in kg_path[user_id][REL_ID_GIVEN_GOOD_RATING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Path generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path_from_entity(entity_id, depth, n_sample_for_each_relation=2):\n",
    "    \n",
    "    generated_paths = []\n",
    "    for relation in kg_path[entity_id]:\n",
    "        \n",
    "        tails = kg_path[entity_id][relation]        \n",
    "        sample_idx = randint(0, len(tails), n_sample_for_each_relation)\n",
    "        sampled_tails = [tails[x] for x in sample_idx]\n",
    "        \n",
    "        for sampled_tail in sampled_tails:\n",
    "            \n",
    "            # Recurse until last layer - 1 is enough\n",
    "            if depth > 2 :\n",
    "                future_paths = generate_path_from_entity(sampled_tail, depth-1, n_sample_for_each_relation)\n",
    "                generated_paths += [\"{} {} {}\".format(relation, sampled_tail, x) for x in future_paths]\n",
    "            else:\n",
    "                generated_paths.append(\"{} {}\".format(relation, sampled_tail))\n",
    "    \n",
    "    return generated_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _split_dataset_positive_negative(path_dataset, sampling_negative_ratio=0.15):\n",
    "    \n",
    "    positive_dataset = []\n",
    "    negative_dataset = []\n",
    "    \n",
    "    for line in (path_dataset):\n",
    "        split = line.split()\n",
    "        user_id = split[0]\n",
    "        potential_movie = split[-1]\n",
    "\n",
    "        if is_user_rated_potential_movie(user_id, potential_movie) :\n",
    "            positive_dataset.append(line.strip() + \" 1\")\n",
    "        else:\n",
    "            if random_sample() < sampling_negative_ratio:\n",
    "                negative_dataset.append(line.strip() + \" 0\")\n",
    "    \n",
    "    return sorted(positive_dataset), sorted(negative_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_batch(generated_paths, batch_num):\n",
    "    generated_paths = list(dict.fromkeys(generated_paths))        \n",
    "    positive_dataset, negative_dataset = _split_dataset_positive_negative(generated_paths)\n",
    "\n",
    "    save_list_to_file(positive_dataset, \"positive_path_{}.txt\".format(batch_num))\n",
    "    save_list_to_file(negative_dataset, \"negative_path_{}.txt\".format(batch_num))\n",
    "    print(\"saved batch {}\".format(batch_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "# > User-item path generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_num = 0\n",
    "generated_paths = []\n",
    "for line in tqdm(file_ratings_re):\n",
    "    \n",
    "    # Checkpoint every 5m paths\n",
    "    if len(generated_paths) > 5000000:        \n",
    "        _save_batch(generated_paths, batch_num)\n",
    "        batch_num += 1\n",
    "        generated_paths = []\n",
    "    \n",
    "    start_user_id, start_movie_id, rating = line.split(',')[:3]\n",
    "    start_user_entity_id = str(USER_ENTITY_ID_PADDING + int(start_user_id))\n",
    "    \n",
    "    # skip if movie is bad\n",
    "    if float(rating) < RATING_THRESHOLD :\n",
    "        continue\n",
    "        \n",
    "    future_paths = generate_path_from_entity(start_movie_id, depth=3)\n",
    "    generated_paths += [\"{} {} {} {}\".format(start_user_entity_id, REL_ID_GIVEN_GOOD_RATING, start_movie_id, x) for x in future_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Save other vocabulary things required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_entity_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entity = [x.strip().split() for x in file_moviesIdx]\n",
    "all_entity += [x.strip().split() for x in file_entities]\n",
    "all_entity = [\"\\t\".join(x) for x in all_entity]\n",
    "\n",
    "save_list_to_file(all_entity, \"all_entity_id.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_relation_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation = sorted(relation_id_to_name.items()) # sorted by key, return a list of tuples\n",
    "all_relation = [\"{}\\t{}\".format(x[1], x[0]) for x in relation] # unpack a list of pairs into two tuples\n",
    "\n",
    "all_relation += [\n",
    "    \"#UNK_RELATION\\t200028\",\n",
    "    \"#PAD_TOKEN\\t200029\",\n",
    "    \"#END_RELATION\\t200030\",\n",
    "]\n",
    "\n",
    "save_list_to_file(all_relation, \"all_relation_id.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity_to_type.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All entity type (including movie alr)\n",
    "all_types = [x.strip() for x in file_types]\n",
    "\n",
    "# All user\n",
    "n_users = int(file_ratings_re[-1].split(',')[0])\n",
    "all_types += [\"u{}\\tUser\".format(i) for i in range(1, n_users + 1)]\n",
    "\n",
    "save_list_to_file(all_types, \"entity_to_type.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entity_type_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_type = [\n",
    "    \"Category\",\n",
    "    \"Company\",\n",
    "    \"Country\",\n",
    "    \"Genre\",\n",
    "    \"Movie\",\n",
    "    \"Person\",\n",
    "    \"User\",\n",
    "    \n",
    "    \"#PAD_TOKEN\",\n",
    "    \"#UNK_ENTITY_TYPE\",\n",
    "]\n",
    "\n",
    "entity_type_to_id = {}\n",
    "entity_type_with_id = []\n",
    "\n",
    "for i in range(0, len(list_of_type)):\n",
    "    entity_type = list_of_type[i]\n",
    "    entity_type_to_id[entity_type] = i\n",
    "    entity_type_with_id.append(\"{}\\t{}\".format(entity_type, i))\n",
    "    \n",
    "save_list_to_file(entity_type_with_id, \"entity_type_id.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Format data for fit own-KPRN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_from_entity_id(entity_id):\n",
    "    \n",
    "    if int(entity_id) > USER_ENTITY_ID_PADDING:\n",
    "        return \"User\"\n",
    "    \n",
    "    elif entity_id_to_name[entity_id] in movie_title_to_entity_type:\n",
    "        return movie_title_to_entity_type[entity_id_to_name[entity_id]]\n",
    "    \n",
    "    else:\n",
    "        return \"#UNK_ENTITY_TYPE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "END_RELATION_ID = 200030\n",
    "BATCH_COUNT = 72\n",
    "\n",
    "def _reformat_paths(paths):\n",
    "\n",
    "    result_paths = []\n",
    "    for line in paths:\n",
    "        e1, r1, e2, r2, e3, r3, e4, label = line.strip().split()\n",
    "\n",
    "        t1 = entity_type_to_id[get_type_from_entity_id(e1)]\n",
    "        t2 = entity_type_to_id[get_type_from_entity_id(e2)]\n",
    "        t3 = entity_type_to_id[get_type_from_entity_id(e3)]\n",
    "        t4 = entity_type_to_id[get_type_from_entity_id(e4)]\n",
    "\n",
    "        r4 = END_RELATION_ID\n",
    "\n",
    "        output = [\n",
    "            \"{} {} {}\".format(e1, t1, r1),\n",
    "            \"{} {} {}\".format(e2, t2, r2),\n",
    "            \"{} {} {}\".format(e3, t3, r3),\n",
    "            \"{} {} {}\".format(e4, t4, r4),\n",
    "            label\n",
    "        ]\n",
    "\n",
    "        output = \"#\".join(output)\n",
    "        result_paths.append(output)\n",
    "        \n",
    "    return result_paths\n",
    "\n",
    "\n",
    "def create_own_format():\n",
    "    for p in ['positive', 'negative']:\n",
    "        for i in tqdm(range(0, BATCH_COUNT)):\n",
    "            sample_paths = open(\"{}_path_{}.txt\".format(p, i), 'r').readlines()\n",
    "            formatted_paths = _reformat_paths(sample_paths)\n",
    "            save_list_to_file(formatted_paths, \"new_{}_path_{}.txt\".format(p, i))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_own_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >> Delete temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for p in ['positive', 'negative']:\n",
    "    for i in range(0, BATCH_COUNT):\n",
    "        try:\n",
    "            os.remove(\"{}_path_{}.txt\".format(p, i))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# > Reasoning part (extra, unrelated)\n",
    "\n",
    "Reading paths as a string version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_paths_filename = \"negative_path_0.txt\"\n",
    "sample_paths = open(sample_paths_filename, 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_entity_name(entity_id):\n",
    "    \n",
    "    if entity_id in entity_id_to_name:\n",
    "        return entity_id_to_name[entity_id]\n",
    "    elif entity_id in relation_id_to_name:\n",
    "        return relation_id_to_name[entity_id]\n",
    "    else:\n",
    "        return \"user_{}\".format(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reasoning_path = []\n",
    "for line in tqdm(sample_paths):\n",
    "    string_line = \" -> \".join([get_entity_name(x) for x in line.split()[:-1]])\n",
    "    reasoning_path.append(string_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reasoning_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "732px",
    "left": "1546px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
